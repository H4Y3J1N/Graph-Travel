{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Amazon\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from sklearn.metrics import f1_score as sk_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset = Amazon(root='data/Amazon', name='Computers')\n",
    "data = dataset[0]\n",
    "\n",
    "data = train_test_split_edges(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[13752, 767], y=[13752], val_pos_edge_index=[2, 12293], test_pos_edge_index=[2, 24586], train_pos_edge_index=[2, 417964], train_neg_adj_mask=[13752, 13752], val_neg_edge_index=[2, 12293], test_neg_edge_index=[2, 24586])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = torch_geometric.nn.GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv_mu = torch_geometric.nn.GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_logvar = torch_geometric.nn.GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(2 * out_channels)\n",
    "        self.bn_mu = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.bn_logvar = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        mu = self.bn_mu(mu)\n",
    "        \n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "        logvar = self.bn_logvar(logvar)\n",
    "        \n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels = 16\n",
    "encoder = Encoder(dataset.num_features, out_channels)\n",
    "model = VGAE(encoder)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data.x, data.train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, data.train_pos_edge_index) + (1 / data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.train_pos_edge_index)\n",
    "    auc, ap = model.test(z, pos_edge_index, neg_edge_index)\n",
    "    \n",
    "    pos_pred = model.decode(z, pos_edge_index).view(-1)\n",
    "    neg_pred = model.decode(z, neg_edge_index).view(-1)\n",
    "    preds = torch.cat([pos_pred, neg_pred])\n",
    "    \n",
    "    pos_label = torch.ones(pos_edge_index.size(1), )\n",
    "    neg_label = torch.zeros(neg_edge_index.size(1), )\n",
    "    labels = torch.cat([pos_label, neg_label])\n",
    "    \n",
    "    preds = (preds > 0.5).float().cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    f1 = sk_f1_score(labels, preds)\n",
    "    return auc, ap, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 128.2774, AUC: 0.6646, AP: 0.6034, F1: 0.6667\n",
      "Epoch: 2, Loss: 20.7686, AUC: 0.5819, AP: 0.5454, F1: 0.6670\n",
      "Epoch: 3, Loss: 17.5908, AUC: 0.5719, AP: 0.5393, F1: 0.6681\n",
      "Epoch: 4, Loss: 14.2298, AUC: 0.5798, AP: 0.5442, F1: 0.6695\n",
      "Epoch: 5, Loss: 18.0012, AUC: 0.5785, AP: 0.5434, F1: 0.6712\n",
      "Epoch: 6, Loss: 16.4956, AUC: 0.5834, AP: 0.5464, F1: 0.6728\n",
      "Epoch: 7, Loss: 14.1026, AUC: 0.5872, AP: 0.5488, F1: 0.6747\n",
      "Epoch: 8, Loss: 11.4815, AUC: 0.5929, AP: 0.5524, F1: 0.6769\n",
      "Epoch: 9, Loss: 11.2207, AUC: 0.5992, AP: 0.5564, F1: 0.6800\n",
      "Epoch: 10, Loss: 9.9730, AUC: 0.6070, AP: 0.5616, F1: 0.6836\n",
      "Epoch: 11, Loss: 11.9367, AUC: 0.6128, AP: 0.5654, F1: 0.6869\n",
      "Epoch: 12, Loss: 9.1552, AUC: 0.6196, AP: 0.5699, F1: 0.6906\n",
      "Epoch: 13, Loss: 10.0544, AUC: 0.6259, AP: 0.5743, F1: 0.6930\n",
      "Epoch: 14, Loss: 10.4375, AUC: 0.6354, AP: 0.5810, F1: 0.6960\n",
      "Epoch: 15, Loss: 8.7407, AUC: 0.6448, AP: 0.5878, F1: 0.6986\n",
      "Epoch: 16, Loss: 8.5230, AUC: 0.6560, AP: 0.5962, F1: 0.7016\n",
      "Epoch: 17, Loss: 10.3619, AUC: 0.6693, AP: 0.6066, F1: 0.7055\n",
      "Epoch: 18, Loss: 8.2331, AUC: 0.6829, AP: 0.6179, F1: 0.7098\n",
      "Epoch: 19, Loss: 9.6434, AUC: 0.6984, AP: 0.6315, F1: 0.7138\n",
      "Epoch: 20, Loss: 8.2481, AUC: 0.7135, AP: 0.6455, F1: 0.7175\n",
      "Epoch: 21, Loss: 7.8800, AUC: 0.7274, AP: 0.6592, F1: 0.7215\n",
      "Epoch: 22, Loss: 7.8382, AUC: 0.7412, AP: 0.6737, F1: 0.7241\n",
      "Epoch: 23, Loss: 7.7043, AUC: 0.7545, AP: 0.6883, F1: 0.7273\n",
      "Epoch: 24, Loss: 7.6842, AUC: 0.7666, AP: 0.7021, F1: 0.7302\n",
      "Epoch: 25, Loss: 7.6208, AUC: 0.7786, AP: 0.7162, F1: 0.7326\n",
      "Epoch: 26, Loss: 7.3746, AUC: 0.7899, AP: 0.7301, F1: 0.7349\n",
      "Epoch: 27, Loss: 7.4862, AUC: 0.8003, AP: 0.7431, F1: 0.7368\n",
      "Epoch: 28, Loss: 7.1715, AUC: 0.8100, AP: 0.7560, F1: 0.7381\n",
      "Epoch: 29, Loss: 6.9638, AUC: 0.8170, AP: 0.7655, F1: 0.7387\n",
      "Epoch: 30, Loss: 7.2961, AUC: 0.8246, AP: 0.7756, F1: 0.7384\n",
      "Epoch: 31, Loss: 7.0054, AUC: 0.8309, AP: 0.7847, F1: 0.7384\n",
      "Epoch: 32, Loss: 7.0527, AUC: 0.8357, AP: 0.7917, F1: 0.7395\n",
      "Epoch: 33, Loss: 6.7516, AUC: 0.8377, AP: 0.7946, F1: 0.7401\n",
      "Epoch: 34, Loss: 6.6931, AUC: 0.8401, AP: 0.7977, F1: 0.7422\n",
      "Epoch: 35, Loss: 6.5552, AUC: 0.8414, AP: 0.7996, F1: 0.7444\n",
      "Epoch: 36, Loss: 6.5118, AUC: 0.8426, AP: 0.8008, F1: 0.7472\n",
      "Epoch: 37, Loss: 6.3995, AUC: 0.8445, AP: 0.8028, F1: 0.7507\n",
      "Epoch: 38, Loss: 6.3184, AUC: 0.8458, AP: 0.8039, F1: 0.7542\n",
      "Epoch: 39, Loss: 6.2530, AUC: 0.8472, AP: 0.8049, F1: 0.7574\n",
      "Epoch: 40, Loss: 6.1503, AUC: 0.8490, AP: 0.8062, F1: 0.7603\n",
      "Epoch: 41, Loss: 6.1830, AUC: 0.8508, AP: 0.8069, F1: 0.7636\n",
      "Epoch: 42, Loss: 6.0386, AUC: 0.8536, AP: 0.8094, F1: 0.7674\n",
      "Epoch: 43, Loss: 5.9513, AUC: 0.8559, AP: 0.8110, F1: 0.7707\n",
      "Epoch: 44, Loss: 5.9452, AUC: 0.8575, AP: 0.8120, F1: 0.7738\n",
      "Epoch: 45, Loss: 5.8301, AUC: 0.8584, AP: 0.8118, F1: 0.7762\n",
      "Epoch: 46, Loss: 5.7443, AUC: 0.8594, AP: 0.8121, F1: 0.7782\n",
      "Epoch: 47, Loss: 5.7208, AUC: 0.8599, AP: 0.8119, F1: 0.7800\n",
      "Epoch: 48, Loss: 5.6599, AUC: 0.8600, AP: 0.8115, F1: 0.7811\n",
      "Epoch: 49, Loss: 5.6243, AUC: 0.8598, AP: 0.8106, F1: 0.7825\n",
      "Epoch: 50, Loss: 5.4884, AUC: 0.8598, AP: 0.8104, F1: 0.7830\n",
      "Epoch: 51, Loss: 5.4645, AUC: 0.8599, AP: 0.8103, F1: 0.7832\n",
      "Epoch: 52, Loss: 5.4294, AUC: 0.8600, AP: 0.8104, F1: 0.7837\n",
      "Epoch: 53, Loss: 5.4238, AUC: 0.8612, AP: 0.8122, F1: 0.7840\n",
      "Epoch: 54, Loss: 5.3646, AUC: 0.8628, AP: 0.8145, F1: 0.7844\n",
      "Epoch: 55, Loss: 5.1870, AUC: 0.8646, AP: 0.8173, F1: 0.7849\n",
      "Epoch: 56, Loss: 5.2486, AUC: 0.8662, AP: 0.8194, F1: 0.7854\n",
      "Epoch: 57, Loss: 5.0839, AUC: 0.8687, AP: 0.8230, F1: 0.7857\n",
      "Epoch: 58, Loss: 5.0379, AUC: 0.8706, AP: 0.8262, F1: 0.7861\n",
      "Epoch: 59, Loss: 5.0488, AUC: 0.8712, AP: 0.8271, F1: 0.7866\n",
      "Epoch: 60, Loss: 4.8783, AUC: 0.8712, AP: 0.8271, F1: 0.7875\n",
      "Epoch: 61, Loss: 4.9142, AUC: 0.8709, AP: 0.8265, F1: 0.7882\n",
      "Epoch: 62, Loss: 4.7763, AUC: 0.8709, AP: 0.8267, F1: 0.7881\n",
      "Epoch: 63, Loss: 4.7423, AUC: 0.8717, AP: 0.8278, F1: 0.7878\n",
      "Epoch: 64, Loss: 4.7545, AUC: 0.8733, AP: 0.8298, F1: 0.7878\n",
      "Epoch: 65, Loss: 4.6893, AUC: 0.8746, AP: 0.8314, F1: 0.7875\n",
      "Epoch: 66, Loss: 4.5968, AUC: 0.8750, AP: 0.8319, F1: 0.7879\n",
      "Epoch: 67, Loss: 4.5327, AUC: 0.8751, AP: 0.8321, F1: 0.7881\n",
      "Epoch: 68, Loss: 4.5488, AUC: 0.8759, AP: 0.8332, F1: 0.7881\n",
      "Epoch: 69, Loss: 4.4926, AUC: 0.8771, AP: 0.8345, F1: 0.7881\n",
      "Epoch: 70, Loss: 4.3816, AUC: 0.8776, AP: 0.8350, F1: 0.7880\n",
      "Epoch: 71, Loss: 4.4076, AUC: 0.8780, AP: 0.8355, F1: 0.7878\n",
      "Epoch: 72, Loss: 4.3039, AUC: 0.8783, AP: 0.8356, F1: 0.7876\n",
      "Epoch: 73, Loss: 4.2686, AUC: 0.8788, AP: 0.8364, F1: 0.7872\n",
      "Epoch: 74, Loss: 4.2159, AUC: 0.8797, AP: 0.8378, F1: 0.7872\n",
      "Epoch: 75, Loss: 4.1816, AUC: 0.8802, AP: 0.8387, F1: 0.7872\n",
      "Epoch: 76, Loss: 4.1583, AUC: 0.8807, AP: 0.8395, F1: 0.7876\n",
      "Epoch: 77, Loss: 4.1395, AUC: 0.8810, AP: 0.8404, F1: 0.7886\n",
      "Epoch: 78, Loss: 4.0344, AUC: 0.8812, AP: 0.8413, F1: 0.7895\n",
      "Epoch: 79, Loss: 4.0168, AUC: 0.8812, AP: 0.8421, F1: 0.7900\n",
      "Epoch: 80, Loss: 3.9416, AUC: 0.8811, AP: 0.8424, F1: 0.7901\n",
      "Epoch: 81, Loss: 3.8669, AUC: 0.8808, AP: 0.8425, F1: 0.7904\n",
      "Epoch: 82, Loss: 3.9255, AUC: 0.8810, AP: 0.8432, F1: 0.7908\n",
      "Epoch: 83, Loss: 3.8335, AUC: 0.8812, AP: 0.8438, F1: 0.7910\n",
      "Epoch: 84, Loss: 3.8490, AUC: 0.8816, AP: 0.8446, F1: 0.7913\n",
      "Epoch: 85, Loss: 3.7773, AUC: 0.8818, AP: 0.8450, F1: 0.7916\n",
      "Epoch: 86, Loss: 3.7140, AUC: 0.8820, AP: 0.8457, F1: 0.7918\n",
      "Epoch: 87, Loss: 3.6886, AUC: 0.8833, AP: 0.8476, F1: 0.7922\n",
      "Epoch: 88, Loss: 3.6125, AUC: 0.8846, AP: 0.8495, F1: 0.7923\n",
      "Epoch: 89, Loss: 3.6072, AUC: 0.8858, AP: 0.8514, F1: 0.7922\n",
      "Epoch: 90, Loss: 3.5278, AUC: 0.8872, AP: 0.8537, F1: 0.7927\n",
      "Epoch: 91, Loss: 3.5458, AUC: 0.8889, AP: 0.8564, F1: 0.7925\n",
      "Epoch: 92, Loss: 3.4481, AUC: 0.8903, AP: 0.8588, F1: 0.7926\n",
      "Epoch: 93, Loss: 3.4546, AUC: 0.8922, AP: 0.8615, F1: 0.7922\n",
      "Epoch: 94, Loss: 3.3959, AUC: 0.8937, AP: 0.8636, F1: 0.7923\n",
      "Epoch: 95, Loss: 3.3724, AUC: 0.8945, AP: 0.8650, F1: 0.7924\n",
      "Epoch: 96, Loss: 3.2784, AUC: 0.8952, AP: 0.8659, F1: 0.7925\n",
      "Epoch: 97, Loss: 3.2776, AUC: 0.8959, AP: 0.8668, F1: 0.7926\n",
      "Epoch: 98, Loss: 3.2783, AUC: 0.8958, AP: 0.8670, F1: 0.7928\n",
      "Epoch: 99, Loss: 3.2316, AUC: 0.8953, AP: 0.8662, F1: 0.7934\n",
      "Epoch: 100, Loss: 3.1969, AUC: 0.8948, AP: 0.8658, F1: 0.7936\n",
      "Epoch: 101, Loss: 3.1614, AUC: 0.8941, AP: 0.8651, F1: 0.7939\n",
      "Epoch: 102, Loss: 3.1229, AUC: 0.8931, AP: 0.8638, F1: 0.7942\n",
      "Epoch: 103, Loss: 3.0420, AUC: 0.8922, AP: 0.8629, F1: 0.7944\n",
      "Epoch: 104, Loss: 3.0465, AUC: 0.8914, AP: 0.8620, F1: 0.7944\n",
      "Epoch: 105, Loss: 3.0301, AUC: 0.8908, AP: 0.8617, F1: 0.7945\n",
      "Epoch: 106, Loss: 2.9890, AUC: 0.8904, AP: 0.8613, F1: 0.7947\n",
      "Epoch: 107, Loss: 2.9571, AUC: 0.8907, AP: 0.8620, F1: 0.7947\n",
      "Epoch: 108, Loss: 2.9601, AUC: 0.8912, AP: 0.8631, F1: 0.7948\n",
      "Epoch: 109, Loss: 2.9149, AUC: 0.8911, AP: 0.8633, F1: 0.7947\n",
      "Epoch: 110, Loss: 2.8345, AUC: 0.8917, AP: 0.8644, F1: 0.7948\n",
      "Epoch: 111, Loss: 2.8549, AUC: 0.8929, AP: 0.8665, F1: 0.7946\n",
      "Epoch: 112, Loss: 2.7802, AUC: 0.8943, AP: 0.8689, F1: 0.7943\n",
      "Epoch: 113, Loss: 2.7826, AUC: 0.8967, AP: 0.8726, F1: 0.7941\n",
      "Epoch: 114, Loss: 2.7304, AUC: 0.8987, AP: 0.8756, F1: 0.7941\n",
      "Epoch: 115, Loss: 2.7316, AUC: 0.8999, AP: 0.8777, F1: 0.7938\n",
      "Epoch: 116, Loss: 2.6738, AUC: 0.9013, AP: 0.8800, F1: 0.7936\n",
      "Epoch: 117, Loss: 2.7027, AUC: 0.9017, AP: 0.8806, F1: 0.7938\n",
      "Epoch: 118, Loss: 2.6185, AUC: 0.9017, AP: 0.8805, F1: 0.7940\n",
      "Epoch: 119, Loss: 2.6501, AUC: 0.9017, AP: 0.8808, F1: 0.7936\n",
      "Epoch: 120, Loss: 2.6160, AUC: 0.9019, AP: 0.8810, F1: 0.7936\n",
      "Epoch: 121, Loss: 2.5853, AUC: 0.9027, AP: 0.8818, F1: 0.7936\n",
      "Epoch: 122, Loss: 2.5432, AUC: 0.9028, AP: 0.8820, F1: 0.7933\n",
      "Epoch: 123, Loss: 2.4899, AUC: 0.9026, AP: 0.8816, F1: 0.7935\n",
      "Epoch: 124, Loss: 2.5313, AUC: 0.9026, AP: 0.8815, F1: 0.7935\n",
      "Epoch: 125, Loss: 2.4727, AUC: 0.9025, AP: 0.8815, F1: 0.7937\n",
      "Epoch: 126, Loss: 2.4323, AUC: 0.9022, AP: 0.8812, F1: 0.7941\n",
      "Epoch: 127, Loss: 2.4142, AUC: 0.9023, AP: 0.8814, F1: 0.7942\n",
      "Epoch: 128, Loss: 2.4016, AUC: 0.9016, AP: 0.8805, F1: 0.7939\n",
      "Epoch: 129, Loss: 2.3768, AUC: 0.9013, AP: 0.8803, F1: 0.7939\n",
      "Epoch: 130, Loss: 2.3442, AUC: 0.9014, AP: 0.8803, F1: 0.7941\n",
      "Epoch: 131, Loss: 2.3554, AUC: 0.9021, AP: 0.8814, F1: 0.7940\n",
      "Epoch: 132, Loss: 2.3180, AUC: 0.9027, AP: 0.8824, F1: 0.7939\n",
      "Epoch: 133, Loss: 2.3155, AUC: 0.9028, AP: 0.8828, F1: 0.7941\n",
      "Epoch: 134, Loss: 2.2942, AUC: 0.9033, AP: 0.8839, F1: 0.7940\n",
      "Epoch: 135, Loss: 2.2493, AUC: 0.9034, AP: 0.8845, F1: 0.7942\n",
      "Epoch: 136, Loss: 2.2317, AUC: 0.9038, AP: 0.8856, F1: 0.7942\n",
      "Epoch: 137, Loss: 2.2406, AUC: 0.9043, AP: 0.8864, F1: 0.7941\n",
      "Epoch: 138, Loss: 2.2078, AUC: 0.9051, AP: 0.8877, F1: 0.7944\n",
      "Epoch: 139, Loss: 2.2016, AUC: 0.9060, AP: 0.8890, F1: 0.7942\n",
      "Epoch: 140, Loss: 2.1814, AUC: 0.9066, AP: 0.8898, F1: 0.7941\n",
      "Epoch: 141, Loss: 2.1386, AUC: 0.9077, AP: 0.8914, F1: 0.7942\n",
      "Epoch: 142, Loss: 2.1180, AUC: 0.9087, AP: 0.8927, F1: 0.7942\n",
      "Epoch: 143, Loss: 2.1012, AUC: 0.9100, AP: 0.8946, F1: 0.7937\n",
      "Epoch: 144, Loss: 2.0757, AUC: 0.9105, AP: 0.8954, F1: 0.7936\n",
      "Epoch: 145, Loss: 2.0998, AUC: 0.9106, AP: 0.8959, F1: 0.7938\n",
      "Epoch: 146, Loss: 2.0592, AUC: 0.9105, AP: 0.8961, F1: 0.7939\n",
      "Epoch: 147, Loss: 2.0221, AUC: 0.9109, AP: 0.8970, F1: 0.7939\n",
      "Epoch: 148, Loss: 2.0414, AUC: 0.9112, AP: 0.8975, F1: 0.7943\n",
      "Epoch: 149, Loss: 2.0351, AUC: 0.9119, AP: 0.8983, F1: 0.7945\n",
      "Epoch: 150, Loss: 1.9823, AUC: 0.9120, AP: 0.8989, F1: 0.7949\n",
      "Epoch: 151, Loss: 1.9506, AUC: 0.9122, AP: 0.8994, F1: 0.7951\n",
      "Epoch: 152, Loss: 1.9747, AUC: 0.9124, AP: 0.8997, F1: 0.7953\n",
      "Epoch: 153, Loss: 1.9573, AUC: 0.9121, AP: 0.8993, F1: 0.7956\n",
      "Epoch: 154, Loss: 1.9145, AUC: 0.9119, AP: 0.8993, F1: 0.7958\n",
      "Epoch: 155, Loss: 1.9281, AUC: 0.9122, AP: 0.8999, F1: 0.7961\n",
      "Epoch: 156, Loss: 1.9351, AUC: 0.9121, AP: 0.8999, F1: 0.7960\n",
      "Epoch: 157, Loss: 1.8895, AUC: 0.9124, AP: 0.9002, F1: 0.7960\n",
      "Epoch: 158, Loss: 1.8879, AUC: 0.9128, AP: 0.9008, F1: 0.7959\n",
      "Epoch: 159, Loss: 1.8796, AUC: 0.9132, AP: 0.9013, F1: 0.7957\n",
      "Epoch: 160, Loss: 1.8570, AUC: 0.9132, AP: 0.9014, F1: 0.7956\n",
      "Epoch: 161, Loss: 1.8427, AUC: 0.9136, AP: 0.9019, F1: 0.7954\n",
      "Epoch: 162, Loss: 1.8219, AUC: 0.9139, AP: 0.9026, F1: 0.7959\n",
      "Epoch: 163, Loss: 1.8183, AUC: 0.9142, AP: 0.9035, F1: 0.7960\n",
      "Epoch: 164, Loss: 1.8106, AUC: 0.9143, AP: 0.9039, F1: 0.7967\n",
      "Epoch: 165, Loss: 1.8148, AUC: 0.9143, AP: 0.9042, F1: 0.7971\n",
      "Epoch: 166, Loss: 1.7982, AUC: 0.9143, AP: 0.9048, F1: 0.7976\n",
      "Epoch: 167, Loss: 1.7946, AUC: 0.9146, AP: 0.9056, F1: 0.7978\n",
      "Epoch: 168, Loss: 1.7756, AUC: 0.9147, AP: 0.9060, F1: 0.7980\n",
      "Epoch: 169, Loss: 1.7551, AUC: 0.9150, AP: 0.9068, F1: 0.7981\n",
      "Epoch: 170, Loss: 1.7521, AUC: 0.9155, AP: 0.9076, F1: 0.7977\n",
      "Epoch: 171, Loss: 1.7442, AUC: 0.9158, AP: 0.9082, F1: 0.7976\n",
      "Epoch: 172, Loss: 1.7324, AUC: 0.9161, AP: 0.9086, F1: 0.7976\n",
      "Epoch: 173, Loss: 1.7191, AUC: 0.9162, AP: 0.9087, F1: 0.7976\n",
      "Epoch: 174, Loss: 1.7370, AUC: 0.9160, AP: 0.9084, F1: 0.7981\n",
      "Epoch: 175, Loss: 1.7166, AUC: 0.9157, AP: 0.9082, F1: 0.7983\n",
      "Epoch: 176, Loss: 1.6968, AUC: 0.9153, AP: 0.9079, F1: 0.7981\n",
      "Epoch: 177, Loss: 1.6830, AUC: 0.9150, AP: 0.9078, F1: 0.7982\n",
      "Epoch: 178, Loss: 1.6800, AUC: 0.9151, AP: 0.9079, F1: 0.7982\n",
      "Epoch: 179, Loss: 1.6636, AUC: 0.9159, AP: 0.9089, F1: 0.7983\n",
      "Epoch: 180, Loss: 1.6546, AUC: 0.9163, AP: 0.9094, F1: 0.7981\n",
      "Epoch: 181, Loss: 1.6311, AUC: 0.9171, AP: 0.9103, F1: 0.7982\n",
      "Epoch: 182, Loss: 1.6299, AUC: 0.9177, AP: 0.9110, F1: 0.7977\n",
      "Epoch: 183, Loss: 1.6365, AUC: 0.9176, AP: 0.9112, F1: 0.7977\n",
      "Epoch: 184, Loss: 1.6115, AUC: 0.9179, AP: 0.9118, F1: 0.7979\n",
      "Epoch: 185, Loss: 1.6121, AUC: 0.9179, AP: 0.9120, F1: 0.7978\n",
      "Epoch: 186, Loss: 1.5973, AUC: 0.9180, AP: 0.9124, F1: 0.7979\n",
      "Epoch: 187, Loss: 1.5923, AUC: 0.9180, AP: 0.9126, F1: 0.7979\n",
      "Epoch: 188, Loss: 1.6053, AUC: 0.9181, AP: 0.9130, F1: 0.7977\n",
      "Epoch: 189, Loss: 1.5971, AUC: 0.9181, AP: 0.9131, F1: 0.7977\n",
      "Epoch: 190, Loss: 1.5842, AUC: 0.9180, AP: 0.9129, F1: 0.7979\n",
      "Epoch: 191, Loss: 1.5621, AUC: 0.9178, AP: 0.9129, F1: 0.7978\n",
      "Epoch: 192, Loss: 1.5655, AUC: 0.9180, AP: 0.9130, F1: 0.7981\n",
      "Epoch: 193, Loss: 1.5535, AUC: 0.9183, AP: 0.9133, F1: 0.7982\n",
      "Epoch: 194, Loss: 1.5538, AUC: 0.9188, AP: 0.9137, F1: 0.7987\n",
      "Epoch: 195, Loss: 1.5375, AUC: 0.9193, AP: 0.9140, F1: 0.7985\n",
      "Epoch: 196, Loss: 1.5548, AUC: 0.9200, AP: 0.9145, F1: 0.7984\n",
      "Epoch: 197, Loss: 1.5357, AUC: 0.9204, AP: 0.9148, F1: 0.7980\n",
      "Epoch: 198, Loss: 1.5453, AUC: 0.9207, AP: 0.9149, F1: 0.7976\n",
      "Epoch: 199, Loss: 1.5148, AUC: 0.9208, AP: 0.9149, F1: 0.7979\n",
      "Epoch: 200, Loss: 1.5184, AUC: 0.9210, AP: 0.9151, F1: 0.7977\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    auc, ap, f1 = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss:.4f}, AUC: {auc:.4f}, AP: {ap:.4f}, F1: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNNs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
