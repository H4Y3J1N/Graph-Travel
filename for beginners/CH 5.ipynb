{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모든 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.datasets import FacebookPagePage\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as BCEWithLogits\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing, GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges, negative_sampling\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FacebookPagePage(root=\".\")\n",
    "\n",
    "data = dataset[0] \n",
    "if data.edge_attr is None:\n",
    "    data.edge_attr = torch.ones(data.edge_index.shape[1], )\n",
    "# Edge 분할\n",
    "data = train_test_split_edges(data)\n",
    "\n",
    "print(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "        self.lin = torch.nn.Linear(in_channels * 2, 1)\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = torch.cat([x_i, x_j], dim=-1)\n",
    "        x = self.lin(x)\n",
    "        return torch.sigmoid(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout=0.1057085140333304, activation_func=torch.nn.SELU()):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.activation_func = activation_func\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # 입력 계층\n",
    "        self.layers.append(GCNConv(in_channels, hidden_channels))\n",
    "\n",
    "        # 숨겨진 계층\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        # 출력 계층\n",
    "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = self.activation_func(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.layers[-1](x, edge_index) # 마지막 계층에서는 활성화 함수와 dropout을 적용하지 않습니다.\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    link_predictor.train()\n",
    "\n",
    "    pos_edge_index = data.train_pos_edge_index \n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=pos_edge_index,\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=pos_edge_index.size(1),\n",
    "    )\n",
    "    neg_edge_index = neg_edge_index \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model(data.x, pos_edge_index)  \n",
    "    pos_pred = link_predictor(z[pos_edge_index[0]], z[pos_edge_index[1]])\n",
    "    neg_pred = link_predictor(z[neg_edge_index[0]], z[neg_edge_index[1]])\n",
    "\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    pos_loss = loss_func(pos_pred, torch.ones(pos_edge_index.shape[1], device=device))  \n",
    "    neg_loss = loss_func(neg_pred, torch.zeros(neg_edge_index.shape[1], device=device)) \n",
    "    loss = pos_loss + neg_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    link_predictor.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.val_pos_edge_index)\n",
    "        pos_pred = link_predictor(z[data.val_pos_edge_index[0]], z[data.val_pos_edge_index[1]])\n",
    "        neg_pred = link_predictor(z[data.val_neg_edge_index[0]], z[data.val_neg_edge_index[1]])\n",
    "\n",
    "        pos_loss = F.binary_cross_entropy(pos_pred, torch.ones(data.val_pos_edge_index.size(1)))\n",
    "        neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros(data.val_neg_edge_index.size(1)))\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        labels = torch.cat([torch.ones(data.val_pos_edge_index.size(1)), torch.zeros(data.val_neg_edge_index.size(1))]).cpu().numpy()\n",
    "        preds = torch.cat([pos_pred, neg_pred]).cpu().numpy()\n",
    "        preds_class = (preds > 0.5).astype(int)\n",
    "\n",
    "        precision = precision_score(labels, preds_class)\n",
    "        recall = recall_score(labels, preds_class)\n",
    "        f1 = f1_score(labels, preds_class)\n",
    "        roc_auc = roc_auc_score(labels, preds)\n",
    "\n",
    "    return loss.item(), precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "out_channels = 8\n",
    "model = GCN(in_channels=data.num_node_features, hidden_channels=16, out_channels=out_channels, num_layers=3, activation_func=torch.nn.SELU())\n",
    "link_predictor = LinkPredictor(out_channels)  # out_channels와 동일하게 설정\n",
    "optimizer = torch.optim.AdamW(list(model.parameters()) + list(link_predictor.parameters()), lr=0.019324891788175074, weight_decay=0.02335770445049197)\n",
    "\n",
    "for epoch in range(6):\n",
    "    train_loss = train()\n",
    "    val_loss, val_precision, val_recall, val_f1, val_roc_auc = validate()\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Precision: {val_precision}, Recall: {val_recall}, F1: {val_f1}, ROC AUC: {val_roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    link_predictor.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = model(data.x, data.test_pos_edge_index)\n",
    "        pos_pred = link_predictor(z[data.test_pos_edge_index[0]], z[data.test_pos_edge_index[1]])\n",
    "        neg_pred = link_predictor(z[data.test_neg_edge_index[0]], z[data.test_neg_edge_index[1]])\n",
    "\n",
    "        pos_loss = F.binary_cross_entropy(pos_pred, torch.ones(data.test_pos_edge_index.size(1)))\n",
    "        neg_loss = F.binary_cross_entropy(neg_pred, torch.zeros(data.test_neg_edge_index.size(1)))\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        # 실제 positive, negative edge의 라벨\n",
    "        labels = torch.cat([torch.ones(data.test_pos_edge_index.size(1)), torch.zeros(data.test_neg_edge_index.size(1))]).cpu().numpy()\n",
    "        \n",
    "        # 예측된 확률\n",
    "        preds = torch.cat([pos_pred, neg_pred]).cpu().numpy()\n",
    "\n",
    "        # 이진 분류 문제에서 확률을 기준으로 클래스를 결정\n",
    "        preds_class = (preds > 0.65).astype(int)\n",
    "\n",
    "        precision = precision_score(labels, preds_class)\n",
    "        recall = recall_score(labels, preds_class)\n",
    "        f1 = f1_score(labels, preds_class)\n",
    "        roc_auc = roc_auc_score(labels, preds)\n",
    "        accuracy = accuracy_score(labels, preds_class)\n",
    "        confusion_mat = confusion_matrix(labels, preds_class)\n",
    "\n",
    "    print(\"Test Loss: {:.4f}\".format(loss.item()))\n",
    "    print(\"Precision: {:.4f}\".format(precision))\n",
    "    print(\"Recall: {:.4f}\".format(recall))\n",
    "    print(\"F1 Score: {:.4f}\".format(f1))\n",
    "    print(\"ROC AUC Score: {:.4f}\".format(roc_auc))\n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "\n",
    "    return loss.item(), precision, recall, f1, roc_auc, accuracy, confusion_mat, labels, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, precision, recall, f1, roc_auc, accuracy, confusion_mat, labels, preds = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 시각화\n",
    "# Test Loss, Precision, Recall, F1 Score, ROC AUC Score, Accuracy\n",
    "test_loss = 2.7366\n",
    "precision = 0.6741\n",
    "recall = 0.6320\n",
    "f1_score = 0.6524\n",
    "roc_auc = 0.6912\n",
    "accuracy =  0.6632\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_mat = np.array([[11863, 5219], [6286, 10796]])\n",
    "\n",
    "# Plotting Precision, Recall, F1 Score, ROC AUC Score, Accuracy\n",
    "labels = ['Precision', 'Recall', 'F1 Score', 'ROC AUC Score', 'Accuracy']\n",
    "scores = [precision, recall, f1_score, roc_auc, accuracy]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(labels, scores, color='skyblue')\n",
    "plt.ylim(0, 1)  # Set y-axis limit between 0 and 1\n",
    "plt.title('Evaluation Metrics')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.show()\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 튜닝\n",
    "# 랜덤 시드 고정\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def objective(params):\n",
    "    # Hyperparameters\n",
    "    num_layers = int(params['num_layers'])\n",
    "    hidden_channels = int(params['hidden_channels'])\n",
    "    dropout = params['dropout']\n",
    "    lr = params['lr']\n",
    "    weight_decay = params['weight_decay']\n",
    "\n",
    "    # 모델 생성\n",
    "    model = GCN(in_channels=data.num_node_features, hidden_channels=hidden_channels, out_channels=8, num_layers=num_layers, activation_func=F.leaky_relu, dropout=dropout)\n",
    "    link_predictor = LinkPredictor(8)\n",
    "    optimizer = optim.AdamW(list(model.parameters()) + list(link_predictor.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Early stopping\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Train and validate\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(20):\n",
    "        train_loss = train()\n",
    "        val_loss, val_precision, val_recall, val_f1, val_roc_auc = validate()\n",
    "        print(f\"Epoch: {epoch+1}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "        elif (epoch - best_epoch) == patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        'loss': -val_roc_auc,\n",
    "        'status': STATUS_OK,\n",
    "        'eval_time': time.time(),\n",
    "        'other_stuff': {'train_losses': train_losses, 'val_losses': val_losses}  # Return more information\n",
    "        }\n",
    "\n",
    "# Search space\n",
    "space = {\n",
    "    'num_layers': hp.quniform('num_layers', 2, 4, 1),\n",
    "    'hidden_channels': hp.quniform('hidden_channels', 8, 16, 1),\n",
    "    'dropout': hp.uniform('dropout', 0, 0.5),\n",
    "    'lr': hp.loguniform('lr', -5, -2),\n",
    "    'weight_decay': hp.loguniform('weight_decay', -5, -2)\n",
    "}\n",
    "\n",
    "# optimization 실행\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
