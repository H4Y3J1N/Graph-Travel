{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_RNN_structure(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_size, output_size, num_layers, is_dilation=True, is_bn=True):\n",
    "        super(Graph_RNN_structure, self).__init__()\n",
    "        \n",
    "        # Model configuration\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.is_bn = is_bn\n",
    "\n",
    "        # Model layers\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if is_dilation:\n",
    "            self.conv_block = nn.ModuleList([nn.Conv1d(hidden_size, hidden_size, kernel_size=3, dilation=2**i, padding=2**i) for i in range(num_layers-1)])\n",
    "        else:\n",
    "            self.conv_block = nn.ModuleList([nn.Conv1d(hidden_size, hidden_size, kernel_size=3, dilation=1, padding=1) for i in range(num_layers-1)])\n",
    "        \n",
    "        self.bn_block = nn.ModuleList([nn.BatchNorm1d(hidden_size) for i in range(num_layers-1)])\n",
    "        self.conv_out = nn.Conv1d(hidden_size, 1, kernel_size=3, dilation=1, padding=1)\n",
    "\n",
    "        self.linear_transition = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.hidden_all = []\n",
    "\n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
    "                init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            if isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def init_hidden(self, length=None):\n",
    "        if length is None:\n",
    "            return Variable(torch.ones(self.batch_size, self.hidden_size, 1)).cuda()\n",
    "        else:\n",
    "            return [Variable(torch.ones(self.batch_size, self.hidden_size, 1)).cuda() for _ in range(length)]\n",
    "\n",
    "    def forward(self, x, teacher_forcing, temperature=0.5, bptt=True, bptt_len=20, flexible=True, max_prev_node=100):\n",
    "        hidden_all_cat = torch.cat(self.hidden_all, dim=2)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            hidden_all_cat = self.conv_block[i](hidden_all_cat)\n",
    "            if self.is_bn:\n",
    "                hidden_all_cat = self.bn_block[i](hidden_all_cat)\n",
    "            hidden_all_cat = self.relu(hidden_all_cat)\n",
    "\n",
    "        x_pred = self.conv_out(hidden_all_cat)\n",
    "        x_pred_sample = F.sigmoid(x_pred)\n",
    "        thresh = 0.5\n",
    "        x_thresh = Variable(torch.ones(x_pred_sample.size(0), x_pred_sample.size(1), x_pred_sample.size(2)) * thresh).cuda()\n",
    "        x_pred_sample_long = torch.gt(x_pred_sample, x_thresh).long()\n",
    "\n",
    "        if teacher_forcing:\n",
    "            hidden_all_cat_select = hidden_all_cat * x\n",
    "            x_sum = torch.sum(x, dim=2, keepdim=True).float()\n",
    "        else:\n",
    "            hidden_all_cat_select = hidden_all_cat * x_pred_sample\n",
    "            x_sum = torch.sum(x_pred_sample_long, dim=2, keepdim=True).float()\n",
    "\n",
    "        hidden_new = torch.sum(hidden_all_cat_select, dim=2, keepdim=True) / x_sum\n",
    "        hidden_new = self.linear_transition(hidden_new.permute(0, 2, 1))\n",
    "        hidden_new = hidden_new.permute(0, 2, 1)\n",
    "\n",
    "        if flexible:\n",
    "            if teacher_forcing:\n",
    "                x_id = torch.min(torch.nonzero(torch.squeeze(x.data)))\n",
    "                self.hidden_all = self.hidden_all[x_id:]\n",
    "            else:\n",
    "                x_id = torch.min(torch.nonzero(torch.squeeze(x_pred_sample_long.data)))\n",
    "                start = max(len(self.hidden_all)-max_prev_node+1, x_id)\n",
    "                self.hidden_all = self.hidden_all[start:]\n",
    "        else:\n",
    "            self.hidden_all = self.hidden_all[1:]\n",
    "\n",
    "        self.hidden_all.append(hidden_new)\n",
    "\n",
    "        return x_pred, x_pred_sample\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
